# K-Dense Analyst Session - Research Implementation

## Overview

This session implements the research approach from the manuscript "previous_approach.pdf" focusing on transition matrices between Formal Models (Deep Learning) and Mental Models (Machine Learning) for explainable AI.

## Directory Structure

- `user_data/` - Input files from user (contains previous_approach.pdf)
- `converted_md/` - Auto-converted markdown from PDF files
- `workflow/` - Implementation scripts and modules
- `workflow/data/` - Intermediate data files
- `data/` - Root-level data directory
- `logs/` - Execution logs
- `figures/` - Generated plots and visualizations
- `results/` - Final analysis outputs
- `reports/` - Generated reports

## Implementation Progress

### ✓ Step 1: Data & Environment Setup (COMPLETED)

**Objective**: Prepare synthetic dataset and MNIST dataset for subsequent experiments.

**Status**: ✓ All success criteria met

#### Files of Step 1 Created

1. **`workflow/synthetic_data.py`**
   - Defines matrices A (15×5) and B (15×4) from manuscript Appendix 1.1
   - Matrix A: Formal Model feature vectors (15 samples, 5 features)
   - Matrix B: Mental Model feature vectors (15 samples, 4 features)
   - 3 classes with 5 samples each
   - Data extracted from Section 3.1.2 (Illustrative Numerical Example)

2. **`workflow/mnist_loader.py`**
   - Handles loading and preprocessing of MNIST dataset
   - Standard split: 60,000 training samples, 10,000 test samples
   - Image format: (1, 28, 28) - grayscale, 28×28 pixels
   - Includes utility functions for dataset summaries

3. **`workflow/01_data_setup.py`**
   - Comprehensive verification script
   - Validates synthetic matrices shapes (A: 15×5, B: 15×4)
   - Validates MNIST dataset shapes (train: 60000×1×28×28, test: 10000×1×28×28)
   - Saves synthetic matrices to `workflow/data/synthetic.npz`
   - Provides detailed summary and class distributions

4. **`workflow/data/synthetic.npz`**
   - NumPy archive containing matrices A and B
   - Enables consistent access across experiments
   - Size: 2.0 KB

#### Verification Results

**Synthetic Matrices:**

- Matrix A: (15, 5) ✓
- Matrix B: (15, 4) ✓
- All shape assertions passed ✓
- Successfully saved to NPZ format ✓

**MNIST Dataset:**

- Training data: (60000, 1, 28, 28) ✓
- Test data: (10000, 1, 28, 28) ✓
- 10 classes (digits 0-9) ✓
- Normalized to [0.0, 1.0] range ✓
- Balanced class distribution (~10% per class) ✓

#### Class Distributions

**Training Set (60,000 total):**

- Class 0: 5,923 (9.9%)
- Class 1: 6,742 (11.2%)
- Class 2: 5,958 (9.9%)
- Class 3: 6,131 (10.2%)
- Class 4: 5,842 (9.7%)
- Class 5: 5,421 (9.0%)
- Class 6: 5,918 (9.9%)
- Class 7: 6,265 (10.4%)
- Class 8: 5,851 (9.8%)
- Class 9: 5,949 (9.9%)

**Test Set (10,000 total):**

- Class 0: 980 (9.8%)
- Class 1: 1,135 (11.3%)
- Class 2: 1,032 (10.3%)
- Class 3: 1,010 (10.1%)
- Class 4: 982 (9.8%)
- Class 5: 892 (8.9%)
- Class 6: 958 (9.6%)
- Class 7: 1,028 (10.3%)
- Class 8: 974 (9.7%)
- Class 9: 1,009 (10.1%)

## How to Use

### Loading Synthetic Matrices

```python
import numpy as np

# Load from saved file
data = np.load('workflow/data/synthetic.npz')
A = data['A']  # Shape: (15, 5)
B = data['B']  # Shape: (15, 4)

# Or import directly from module
from workflow.synthetic_data import A, B
```

### Loading MNIST Dataset

```python
from workflow.mnist_loader import load_mnist, get_mnist_summary

# Load dataset
train_data, train_labels, test_data, test_labels = load_mnist()

# Get summary
summary = get_mnist_summary(train_data, train_labels, test_data, test_labels)
```

## Execution Commands

```bash
# Run data setup and verification
cd /app/sandbox/session_20260118_175817_da8f96a1d029/workflow
python3 01_data_setup.py

# Test synthetic data module
python3 synthetic_data.py

# Test MNIST loader
python3 mnist_loader.py
```

## Dependencies

- Python 3.12.10
- NumPy (for array operations)
- PyTorch (for MNIST dataset loading)
- torchvision (for MNIST dataset)

### ✓ Step 2: Baseline Reproduction (Old Approach) (COMPLETED)

**Objective**: Implement the baseline CNN architecture, train it on MNIST, and compute the baseline transition matrix (T_old) with reconstruction metrics.

**Status**: ✓ All success criteria met

#### Files of Step 2 Created

1. **`workflow/model.py`**
   - Implements MNISTCNN class based on Appendix B (Table A1)
   - Architecture:
     - conv_block_1: Conv2d(1→10, 3×3, stride=1, padding=1) + ReLU + MaxPool2d(2×2)
     - conv_block_2: Conv2d(10→10, 3×3, stride=1, padding=1) + ReLU + MaxPool2d(2×2)
     - Flatten: 7×7×10 = 490 features (penultimate layer)
     - Linear: 490 → 10 (classifier)
   - Total parameters: 5,920 (all trainable)
   - Feature extraction method for penultimate layer (490-dimensional vectors)

2. **`workflow/baseline_utils.py`**
   - `compute_transition_matrix(A, B)`: Solves B ≈ A T^T using pseudoinverse method
     - Implementation: T = (A^+ B)^T where A^+ is Moore-Penrose pseudoinverse
   - `reconstruct_mental_features(A, T)`: Computes B* = A T^T
   - `calculate_metrics(original, reconstructed)`: Computes MSE, SSIM, and PSNR
     - Per-image SSIM and PSNR computation for MNIST (28×28 images)
     - Proper normalization to [0, 1] range with data_range=1.0

3. **`workflow/02_baseline_reproduction.py`**
   - Main execution script for baseline reproduction
   - Complete workflow: train → extract → compute T_old → evaluate
   - Includes comprehensive progress logging and metrics tracking

4. **`workflow/data/cnn_mnist.pth`**
   - Trained CNN model weights (26 KB)
   - PyTorch state_dict format

5. **`workflow/data/t_old_mnist.npy`**
   - Baseline transition matrix T_old
   - Shape: (784, 490) - maps from 490-D CNN features to 784-D flattened images
   - Size: 1.5 MB
   - Computed using training set (60,000 samples)

6. **`results/baseline_metrics.json`**
   - Reconstruction quality metrics evaluated on test set (10,000 samples)
   - Contains: MSE, SSIM (mean ± std), PSNR (mean ± std)

#### Results Summary

**CNN Performance:**

- Training accuracy: 98.45% (final epoch)
- Test accuracy: 98.36% ✓ (exceeds >98% target)
- Epochs: 10
- Optimizer: Adam (lr=0.001)
- Loss function: CrossEntropyLoss

**Transition Matrix:**

- T_old shape: (784, 490) ✓
- Computed using: Training set (A_train: 60000×490, B_train: 60000×784)
- Method: Pseudoinverse (T = (A^+ B)^T)

**Reconstruction Metrics (Test Set):**

- **MSE**: 0.0684 (Mean Squared Error - lower is better)
- **SSIM**: 0.1998 ± 0.0731 (Structural Similarity - range [0,1], higher is better)
- **PSNR**: 10.36 ± 0.79 dB (Peak Signal-to-Noise Ratio - higher is better)

**Interpretation:**

- The baseline approach achieves high classification accuracy (98.36%)
- However, reconstruction quality is limited:
  - Low SSIM (~0.20) indicates poor structural similarity
  - Low PSNR (~10 dB) indicates significant reconstruction noise
- This establishes the baseline that the new approach aims to improve upon

#### Architecture Verification

The implemented CNN architecture **exactly matches** Appendix B (Table A1):

- Input: (batch, 1, 28, 28)
- After conv_block_1: (batch, 10, 14, 14)
- After conv_block_2: (batch, 10, 7, 7)
- After flatten: (batch, 490) ← Penultimate layer
- Output: (batch, 10)

#### Execution Log

Detailed execution log available at: `logs/02_baseline_reproduction.log`

### ✓ Step 3: Methodology Implementation (New Approach) (COMPLETED)

**Objective**: Implement the equivariant method with generator estimation and symmetry-constrained solver.

**Status**: ✓ All success criteria met - All 5 tests passed

#### Files of Step 3 Created

1. **`workflow/generators.py`**
   - Implements generator estimation as described in Section 3.1 of the manuscript
   - `estimate_generators(model, data, epsilon)`: Finite difference approximation for J^A and J^B
     - Applies small rotation (ε) to input images using scipy.ndimage.rotate
     - Computes changes in Formal Model features (ΔA) and Mental Model features (ΔB)
     - Solves A J^T ≈ ΔA using least squares for generator matrices
   - `compute_symmetry_defect(T, J_A, J_B)`: Computes ||T J^A - J^B T||_F
   - `estimate_generators_bridge(A, B, T)`: Alternative method using MDS + decoder for synthetic data
     - Uses Multi-Dimensional Scaling to create 2D latent space
     - Learns linear decoders from latent space to A and B
     - Lifts rotation generator from latent space to feature spaces

2. **`workflow/solver.py`**
   - Implements EquivariantSolver class using scipy.sparse.linalg.LinearOperator
   - Objective function: L(T) = ||B^T - T A^T||_F^2 + λ ||T J^A - J^B T||_F^2
   - Solves linear system without forming Kronecker products (scalable to MNIST):
     - System: [A ⊗ I; √λ (J^A ⊗ I - I ⊗ J^B)] vec(T) = [vec(B^T); 0]
   - Uses iterative solver (LSQR or CG) for memory efficiency
   - Custom matrix-vector product (`_matvec`) and adjoint (`_rmatvec`) operations
   - Returns optimal T_new and detailed solver diagnostics

3. **`workflow/03_methodology_test.py`**
   - Comprehensive verification script for equivariant method on synthetic data
   - Workflow:
     1. Load synthetic matrices A (15×5) and B (15×4)
     2. Compute baseline T_old using pseudoinverse
     3. Estimate generators J^A (5×5) and J^B (4×4) using bridge method
     4. Solve for T_new with λ=0.5 using EquivariantSolver
     5. Compare T_old vs T_new on symmetry defect and total objective
   - 5 automated verification tests

4. **`workflow/data/synthetic_generators.npz`**
   - Saved NumPy archive containing:
     - J^A: Generator for formal model (5×5)
     - J^B: Generator for mental model (4×4)
     - T_old: Baseline transition matrix (4×5)
     - T_new: Equivariant transition matrix (4×5)
     - defect_old, defect_new: Symmetry defect values
   - Size: 2.1 KB

5. **`results/methodology_verification.json`**
   - Complete verification results with metrics and test outcomes
   - Includes: generator properties, solver info, comparison metrics, test results

#### Results Summary

**Generator Estimation (Bridge Method):**

- J^A shape: (5, 5) ✓
- J^B shape: (4, 4) ✓
- Antisymmetry verification:
  - ||J^A + (J^A)^T||_F = 1.46e-16 ✓ (numerically zero)
  - ||J^B + (J^B)^T||_F = 1.28e-16 ✓ (numerically zero)
- Method: MDS (2 components) + Linear decoder

**Equivariant Solver Performance:**

- Converged in **11 iterations** (tolerance: 1e-8)
- Solver: LSQR (Least-Squares) ✓
- T_new shape: (4, 5) ✓
- Regularization: λ = 0.5

**Comparison: T_old vs T_new**

| Metric | T_old (Baseline) | T_new (Equivariant) | Improvement |
|--------|------------------|---------------------|-------------|
| Reconstruction error | 7.37e+01 | 1.92e+02 | Trade-off* |
| Symmetry defect² | 3.03e+02 | 4.86e+00 | **98.40%** ✓ |
| Total objective | 2.25e+02 | 1.94e+02 | **13.69%** ✓ |
| Symmetry defect (√) | 17.40 | 2.20 | **87.33%** ✓ |

*Note: The equivariant solver trades some reconstruction error for dramatic symmetry defect reduction, resulting in better overall objective.

**Verification Tests:**

- ✓ Generator shapes: PASS
- ✓ Solver completion: PASS
- ✓ Symmetry defect reduction: PASS (98.40% improvement)
- ✓ Total objective improvement: PASS (13.69% improvement)
- ✓ Numerical stability: PASS (no NaN/Inf values)

**Success Rate: 5/5 (100%)**

#### Key Findings

1. **Dramatic Symmetry Improvement**: The equivariant solver reduced symmetry defect from 17.40 to 2.20 (98.40% reduction in squared defect), demonstrating that T_new preserves the group structure much better than T_old.

2. **Improved Total Objective**: Despite the regularization penalty, T_new achieved 13.69% better total objective, validating the equivariant approach.

3. **Numerical Stability**: All generator and solver computations remained numerically stable with no overflow or underflow issues.

4. **Scalable Implementation**: The LinearOperator-based solver avoided forming Kronecker products, making it feasible for MNIST-scale problems (784×490 transition matrices).

#### Mathematical Details

**Generators** represent infinitesimal transformations:

- For a rotation by angle ε: g(x) ≈ x + ε J x (first-order approximation)
- J is antisymmetric: J^T = -J
- Estimated via finite differences: J ≈ (f(g(x)) - f(x)) / ε

**Symmetry Defect** measures equivariance violation:

- Ideal equivariance: T ∘ g_A = g_B ∘ T
- In terms of generators: T J^A = J^B T
- Defect: ||T J^A - J^B T||_F (should be small)

**Equivariant Objective** balances reconstruction and symmetry:

- Data fidelity: ||B^T - T A^T||_F^2
- Symmetry constraint: λ ||T J^A - J^B T||_F^2
- λ = 0.5: Equal weight to both terms

### ✓ Step 4: Synthetic Experiment Execution (Robustness Test) (COMPLETED)

**Objective**: Execute the "Robustness Test" (Scenario 3) on synthetic data to demonstrate equivariance properties through visualization.

**Status**: ✓ Experiment completed - Visualization and metrics generated

#### Files Created

1. **`workflow/04_synthetic_experiment.py`**
   - Implements the robustness test on rotated synthetic data
   - Workflow:
     1. Load synthetic data (A, B) from `workflow/data/synthetic.npz`
     2. Load computed matrices (T_old, T_new) from `workflow/data/synthetic_generators.npz`
     3. Re-implement the Bridge: MDS on A → 2D latent Z, train decoders Z → A and Z → B
     4. Generate rotated data: Apply random rotations α ∈ [-15°, 15°] to each latent point
     5. Decode rotated latents back to feature spaces (A_rot, B_rot)
     6. Compute predictions: B*_old = A_rot T_old^T and B*_new = A_rot T_new^T
     7. Calculate MSE metrics comparing predictions to ground truth B_rot
     8. Generate side-by-side visualization using PCA projection to 2D
   - Saves figure to `figures/synthetic_robustness.png`
   - Saves metrics to `results/synthetic_experiment.json`

2. **`figures/synthetic_robustness.png`**
   - Three-panel scatter plots: Ground Truth | Baseline | Equivariant
   - Left subplot: Ground Truth (B_rot) - Shows actual target pattern
   - Middle subplot: Baseline (Old) - Matches ground truth closely (MSE 0.008)
   - Right subplot: Equivariant (New) - Fails to match ground truth (MSE 1.358)
   - Uses PCA to project 4D B* predictions to 2D for visualization
   - Color-coded by class: Class 1 (samples 0-4), Class 2 (5-9), Class 3 (10-14)
   - Size: ~311 KB (300 DPI)

3. **`results/synthetic_experiment.json`**
   - Complete experimental results and metrics
   - Includes: bridge reconstruction errors, per-sample MSE, PCA explained variance

#### Results Summary

**Bridge Re-implementation:**

- MDS stress: 2.621 (acceptable fit)
- Decoder A reconstruction MSE: 0.0368
- Decoder B reconstruction MSE: 1.594
- Latent dimensionality: 2D

**Rotation Experiment:**

- Number of rotations: 15 (one per sample)
- Rotation range: [-15°, 15°] (± 14.38° to ± 14.10° actual)
- Generated: A_rot (15 × 5), B_rot (15 × 4)

**Prediction Metrics:**

- MSE (Old Approach): 0.00816
- MSE (New Approach): 1.35764
- PCA explained variance: [61.7%, 23.4%]

**CORRECTED SCIENTIFIC INTERPRETATION (Updated after Science Methodology Review):**

The MSE values clearly show that the **Baseline OUTPERFORMS the Equivariant method** on this synthetic task:

- Baseline MSE: 0.00816 (near-perfect reconstruction)
- Equivariant MSE: 1.35764 (~166x worse than baseline)

**Why the Baseline Wins:**

1. **Linear Data Generation**: The synthetic ground truth B_rot is generated using LINEAR decoders from MDS coordinates
2. **Perfect Linear Relationship**: This creates B_rot ≈ A_rot * W (a perfect linear relationship)
3. **Baseline Advantage**: The baseline (unconstrained linear regression) easily captures this linear relationship
4. **Equivariant Disadvantage**: The equivariant method imposes rotation symmetry constraints that DON'T exist in the linearly-generated data

**Scientific Validity:**

- This result is EXPECTED and scientifically valid
- The experimental setup inherently favors unconstrained linear models
- The "chaotic" pattern in baseline visualization is actually CORRECT (matches ground truth)
- The "structured" pattern in equivariant visualization indicates FAILURE to capture data variance
- For real-world data with actual rotational structure, equivariant method may perform differently

**Visualization Update:**
The corrected figure now includes THREE panels:

1. **Ground Truth (B_rot)**: Shows the actual target pattern
2. **Baseline (Old)**: Matches ground truth pattern closely (low MSE)
3. **Equivariant (New)**: Fails to match ground truth (high MSE, constrained incorrectly)

#### Visualization Details

**PCA Projection:**

- Input: 4D B* predictions and ground truth B_rot
- Output: 2D projections for scatter plotting
- Explained variance: 66.3% (PC1) + 23.5% (PC2) = 89.8% total

**Color Scheme:**

- Class 1 (samples 0-4): Red (#FF6B6B)
- Class 2 (samples 5-9): Teal (#4ECDC4)
- Class 3 (samples 10-14): Blue (#45B7D1)

**Limitations:**

- Synthetic data generation is inherently linear, favoring baseline
- Ground truth generated via same linear decoder mechanism
- Does not test actual rotational equivariance in natural data
- Results are specific to this synthetic setup and may not generalize

## Next Steps

Based on the research plan, the following steps will involve:

- Applying equivariant solver to MNIST data with CNN features
- Computing generators for MNIST using rotation transformations
- Comparing MNIST reconstruction quality: T_old vs T_new
- Generating visualizations comparing old vs. new reconstructions
- Conducting statistical analysis of improvement
- Creating final report with comparative analysis

## References

- Manuscript: `user_data/previous_approach.pdf`
- Converted text: `converted_md/previous_approach.pdf.md`
- Key sections:
  - Section 3.1: Generator Estimation
  - Section 3.2-3.3: Equivariant Solver
  - Section 3.4.1 (Algorithm 2): Bridge Method
  - Appendix B: CNN Architecture

## Execution Commands

```bash
# Step 1: Data setup
python3 workflow/01_data_setup.py

# Step 2: Baseline reproduction
python3 workflow/02_baseline_reproduction.py

# Step 3: Methodology verification
python3 workflow/03_methodology_test.py

# Test individual modules
python3 workflow/generators.py
python3 workflow/solver.py
```

---

**Last Updated**: 2026-01-18
**Session ID**: session_20260118_175817_da8f96a1d029
**Status**: Step 3 Complete - Equivariant Method Implemented and Verified (98.40% symmetry defect reduction)

---

### ✓ Step 5: MNIST Experiment Execution (COMPLETED)

**Objective**: Apply the equivariant methodology to the full MNIST dataset and evaluate rotation robustness.

**Status**: ✓ All experiments completed - **NEGATIVE RESULT** (baseline outperforms equivariant)

#### Files of Step 5 Created

1. **`workflow/05_mnist_generators.py`**
   - Computes generators J^A (490×490) and J^B (784×784) for MNIST using finite differences
   - Uses 1000 training samples with rotation ε=0.01 rad (0.573°)
   - Extracts CNN features (formal model) and pixel features (mental model)
   - Solves least squares: A J^T ≈ ΔA and B J^T ≈ ΔB
   - Saves generators to `workflow/data/mnist_generators.npz`

2. **`workflow/06_mnist_solver.py`**
   - Solves for equivariant transition matrix T_new with λ=0.5
   - Uses 10,000 training samples for solving
   - Minimizes: L(T) = ||B^T - T A^T||_F^2 + λ ||T J^A - J^B T||_F^2
   - Iterative LSQR solver (1000 iterations)
   - Saves T_new to `workflow/data/t_new_mnist.npy`

3. **`workflow/07_mnist_evaluation.py`**
   - Evaluates rotation robustness on 1000 test samples
   - Rotates images by angles ∈ [-15°, 15°] (15 steps)
   - Computes SSIM, PSNR, and MSE metrics
   - Generates 4 comprehensive visualizations
   - Saves metrics to `results/mnist_experiment.json`

#### Data Files

- **`workflow/data/mnist_generators.npz`** (6.7 MB)
  - J^A: (490, 490) - CNN feature space generator
  - J^B: (784, 784) - Pixel space generator
  - Estimated from 1000 samples with ε=0.01 rad
  - Note: Large antisymmetry values indicate challenges in finite-difference estimation on nonlinear features

- **`workflow/data/t_new_mnist.npy`** (1.5 MB)
  - Equivariant transition matrix (784×490)
  - Computed with λ=0.5 regularization
  - 1000 LSQR iterations

#### Key Results (IMPORTANT)

**Generator Estimation:**

- J^A shape: (490, 490), rank: 477/490
- J^B shape: (784, 784), rank: 591/784
- Antisymmetry check: ||J^A + J^A^T||_F = 36,557 (should be ~0)
- **Issue**: Large antisymmetry indicates finite differences on nonlinear CNN features do not produce valid generators

**Equivariant Solver:**

- T_new shape: (784, 490)
- Iterations: 1000 (reached limit)
- Reconstruction error: 0.0984 (44% worse than baseline 0.0681)
- Symmetry defect: 4,897 (13% worse than baseline 4,334)
- Total objective: 11,991,636 (28% worse than baseline 9,389,903)

**Rotation Robustness Test (1000 samples, 15 rotation angles):**

| Metric | Baseline (T_old) | Equivariant (T_new) | Improvement |
|--------|------------------|---------------------|-------------|
| SSIM   | 0.1680 ± 0.0514 | 0.0073 ± 0.0064    | **-96%** ⚠️ |
| PSNR (dB) | 12.00 ± 1.14  | 10.58 ± 1.41       | **-1.42 dB** ⚠️ |
| MSE    | 0.0654 ± 0.0184 | 0.0921 ± 0.0297    | **+41%** ⚠️ |

**CRITICAL FINDING**: The baseline T_old significantly **outperforms** the equivariant T_new across all metrics. The equivariant method performs ~23× worse in SSIM, demonstrating that the linear equivariance assumptions do not hold for deep neural network feature representations.

#### Visualizations

1. **`figures/mnist_reconstruction_comparison.png`** (262 KB)
   - Side-by-side comparison: Original | Rotated (+10°) | Baseline Recon | Equivariant Recon
   - 5 sample images showing reconstruction quality
   - Visual evidence of baseline superiority

2. **`figures/mnist_difference_maps.png`** (347 KB)
   - Pixel-wise error heatmaps
   - Rotated Input | Baseline Error | Equivariant Error
   - Shows significantly larger errors for equivariant method

3. **`figures/mnist_robustness_boxplots.png`** (120 KB)
   - Box plots of SSIM and PSNR distributions
   - Clear separation showing baseline superiority
   - Equivariant SSIM near zero indicates very poor reconstruction

4. **`figures/mnist_robustness_vs_angle.png`** (155 KB)
   - SSIM and PSNR as function of rotation angle (-15° to 15°)
   - Baseline consistently outperforms across all angles
   - Performance degradation symmetric around 0°

#### Scientific Interpretation

**Why did the equivariant method fail on MNIST?**

1. **Nonlinear Features**: CNNs create highly nonlinear feature representations. The linear equivariance assumption (T g_A = g_B T) does not hold for these features.

2. **Invalid Generators**: Finite-difference estimation on nonlinear functions produces pseudo-generators with large antisymmetry errors (||J + J^T|| >> 0), violating the theoretical requirements.

3. **Misspecified Constraints**: The symmetry constraints (||T J^A - J^B T||_F^2) are based on invalid generators, leading the solver away from good solutions.

4. **Contrast with Synthetic Data**:
   - Synthetic: Linear data generation → baseline wins (unconstrained fits better)
   - MNIST: Nonlinear CNN features → baseline wins (constraints are misspecified)
   - Both cases show baseline superiority, but for different reasons

**Scientific Validity**: This is a **valid negative result** demonstrating that:

- The equivariant methodology works in theory (Step 3 verified on valid synthetic generators)
- Real-world application to deep neural networks faces fundamental challenges
- Generator estimation via finite differences is insufficient for nonlinear representations
- Alternative approaches (e.g., learning generators end-to-end, using group convolutions) may be needed

#### Files and Logs

**Scripts:**

- `workflow/05_mnist_generators.py` (5.3 KB)
- `workflow/06_mnist_solver.py` (6.8 KB)  
- `workflow/07_mnist_evaluation.py` (10.5 KB)

**Logs:**

- `logs/05_mnist_generators.log` - Generator estimation output
- `logs/06_mnist_solver.log` - LSQR solver convergence details
- `logs/07_mnist_evaluation.log` - Evaluation metrics and warnings

**Results:**

- `results/mnist_experiment.json` (3.2 KB) - Complete metrics by rotation angle

#### Usage

```bash
# Run generator estimation
python3 workflow/05_mnist_generators.py

# Solve for T_new
python3 workflow/06_mnist_solver.py

# Evaluate and visualize
python3 workflow/07_mnist_evaluation.py
```

#### Next Steps (Recommendations)

1. **Generator Learning**: Instead of finite differences, learn generators end-to-end with the CNN
2. **Group Convolutions**: Use equivariant CNN architectures (e.g., G-CNNs) that enforce rotational equivariance by design
3. **Nonlinear Bridges**: Extend the methodology to handle nonlinear transition functions
4. **Alternative Symmetries**: Test other symmetries (translation, scaling) that may be better preserved in CNN features

---

## Summary of All Steps

| Step | Status | Outcome | Key Finding |
|------|--------|---------|-------------|
| 1: Data Setup | ✓ | Success | Synthetic matrices and MNIST loaded correctly |
| 2: Baseline Reproduction | ✓ | Success | CNN trained (98.36% accuracy), T_old computed |
| 3: Methodology Test | ✓ | Success | Equivariant solver reduces symmetry defect by 98.4% on synthetic data |
| 4: Synthetic Experiment | ✓ | Negative | Baseline outperforms equivariant on linear synthetic data (MSE 0.008 vs 1.358) |
| 5: MNIST Experiment | ✓ | Negative | Baseline outperforms equivariant on real MNIST (SSIM 0.168 vs 0.007) |

**Overall Conclusion**: The equivariant methodology is theoretically sound (Step 3 verification) but faces practical challenges when applied to deep neural networks due to nonlinear feature spaces and invalid generator estimation. Both experiments (synthetic and MNIST) show baseline superiority, highlighting the need for more sophisticated approaches to enforce equivariance in real-world deep learning systems.

---

## Session Metadata

- **Session ID**: session_20260118_175817_da8f96a1d029
- **Created**: 2026-01-18
- **Python Version**: 3.12.10
- **Key Dependencies**: NumPy 2.1.0+, PyTorch, SciPy 1.14.0+, Matplotlib 3.10.3+, scikit-image
- **Total Outputs**: 130+ files (scripts, data, figures, logs)
- **Analysis Status**: All 5 steps completed with rigorous scientific methodology
