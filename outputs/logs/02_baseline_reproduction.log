
================================================================================
BASELINE REPRODUCTION (OLD APPROACH)
================================================================================
Objective: Train CNN, compute T_old, and evaluate reconstruction
================================================================================

Using device: cpu

================================================================================
Step 1: Loading MNIST Data
================================================================================
Train data shape: (60000, 1, 28, 28)
Train labels shape: (60000,)
Test data shape: (10000, 1, 28, 28)
Test labels shape: (10000,)
Batch size: 128
Train batches: 469
Test batches: 79
================================================================================

================================================================================
Step 2: Training CNN Model
================================================================================
Model architecture:
MNISTCNN(
  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc): Linear(in_features=490, out_features=10, bias=True)
)

================================================================================
Training CNN on MNIST Dataset
================================================================================
  Epoch 1/10, Batch 50/469, Loss: 1.1527, Time: 0.9s
  Epoch 1/10, Batch 100/469, Loss: 0.4441, Time: 1.9s
  Epoch 1/10, Batch 150/469, Loss: 0.3166, Time: 2.8s
  Epoch 1/10, Batch 200/469, Loss: 0.2887, Time: 3.9s
  Epoch 1/10, Batch 250/469, Loss: 0.2560, Time: 5.0s
  Epoch 1/10, Batch 300/469, Loss: 0.3160, Time: 6.0s
  Epoch 1/10, Batch 350/469, Loss: 0.2053, Time: 6.9s
  Epoch 1/10, Batch 400/469, Loss: 0.2685, Time: 7.8s
  Epoch 1/10, Batch 450/469, Loss: 0.2016, Time: 8.8s

Epoch 1/10 Summary:
  Average Loss: 0.5026
  Accuracy: 85.51%
  Time: 9.2s
--------------------------------------------------------------------------------
  Epoch 2/10, Batch 50/469, Loss: 0.2275, Time: 0.9s
  Epoch 2/10, Batch 100/469, Loss: 0.1896, Time: 1.8s
  Epoch 2/10, Batch 150/469, Loss: 0.1274, Time: 2.8s
  Epoch 2/10, Batch 200/469, Loss: 0.1270, Time: 3.7s
  Epoch 2/10, Batch 250/469, Loss: 0.1559, Time: 4.6s
  Epoch 2/10, Batch 300/469, Loss: 0.0744, Time: 5.7s
  Epoch 2/10, Batch 350/469, Loss: 0.1955, Time: 6.7s
  Epoch 2/10, Batch 400/469, Loss: 0.0962, Time: 8.2s
  Epoch 2/10, Batch 450/469, Loss: 0.1896, Time: 9.6s

Epoch 2/10 Summary:
  Average Loss: 0.1511
  Accuracy: 95.54%
  Time: 10.3s
--------------------------------------------------------------------------------
  Epoch 3/10, Batch 50/469, Loss: 0.0717, Time: 1.8s
  Epoch 3/10, Batch 100/469, Loss: 0.1251, Time: 3.2s
  Epoch 3/10, Batch 150/469, Loss: 0.1073, Time: 4.4s
  Epoch 3/10, Batch 200/469, Loss: 0.1497, Time: 5.7s
  Epoch 3/10, Batch 250/469, Loss: 0.1139, Time: 6.9s
  Epoch 3/10, Batch 300/469, Loss: 0.0526, Time: 7.9s
  Epoch 3/10, Batch 350/469, Loss: 0.1045, Time: 9.0s
  Epoch 3/10, Batch 400/469, Loss: 0.0404, Time: 10.1s
  Epoch 3/10, Batch 450/469, Loss: 0.2330, Time: 11.2s

Epoch 3/10 Summary:
  Average Loss: 0.1063
  Accuracy: 96.86%
  Time: 11.6s
--------------------------------------------------------------------------------
  Epoch 4/10, Batch 50/469, Loss: 0.1354, Time: 1.0s
  Epoch 4/10, Batch 100/469, Loss: 0.1054, Time: 2.3s
  Epoch 4/10, Batch 150/469, Loss: 0.0516, Time: 3.5s
  Epoch 4/10, Batch 200/469, Loss: 0.1435, Time: 4.7s
  Epoch 4/10, Batch 250/469, Loss: 0.0578, Time: 5.9s
  Epoch 4/10, Batch 300/469, Loss: 0.1084, Time: 7.0s
  Epoch 4/10, Batch 350/469, Loss: 0.1596, Time: 8.2s
  Epoch 4/10, Batch 400/469, Loss: 0.1734, Time: 9.4s
  Epoch 4/10, Batch 450/469, Loss: 0.0250, Time: 10.8s

Epoch 4/10 Summary:
  Average Loss: 0.0874
  Accuracy: 97.37%
  Time: 11.3s
--------------------------------------------------------------------------------
  Epoch 5/10, Batch 50/469, Loss: 0.0946, Time: 1.4s
  Epoch 5/10, Batch 100/469, Loss: 0.0204, Time: 2.9s
  Epoch 5/10, Batch 150/469, Loss: 0.0545, Time: 4.3s
  Epoch 5/10, Batch 200/469, Loss: 0.0418, Time: 5.7s
  Epoch 5/10, Batch 250/469, Loss: 0.0314, Time: 7.0s
  Epoch 5/10, Batch 300/469, Loss: 0.0486, Time: 8.4s
  Epoch 5/10, Batch 350/469, Loss: 0.0183, Time: 9.7s
  Epoch 5/10, Batch 400/469, Loss: 0.0732, Time: 11.1s
  Epoch 5/10, Batch 450/469, Loss: 0.1104, Time: 12.6s

Epoch 5/10 Summary:
  Average Loss: 0.0763
  Accuracy: 97.73%
  Time: 13.2s
--------------------------------------------------------------------------------
  Epoch 6/10, Batch 50/469, Loss: 0.0881, Time: 1.5s
  Epoch 6/10, Batch 100/469, Loss: 0.0521, Time: 2.9s
  Epoch 6/10, Batch 150/469, Loss: 0.0435, Time: 4.0s
  Epoch 6/10, Batch 200/469, Loss: 0.1025, Time: 5.2s
  Epoch 6/10, Batch 250/469, Loss: 0.0527, Time: 6.2s
  Epoch 6/10, Batch 300/469, Loss: 0.0152, Time: 7.5s
  Epoch 6/10, Batch 350/469, Loss: 0.0985, Time: 8.6s
  Epoch 6/10, Batch 400/469, Loss: 0.0284, Time: 9.7s
  Epoch 6/10, Batch 450/469, Loss: 0.0447, Time: 10.7s

Epoch 6/10 Summary:
  Average Loss: 0.0675
  Accuracy: 98.03%
  Time: 11.1s
--------------------------------------------------------------------------------
  Epoch 7/10, Batch 50/469, Loss: 0.0598, Time: 1.1s
  Epoch 7/10, Batch 100/469, Loss: 0.0716, Time: 2.3s
  Epoch 7/10, Batch 150/469, Loss: 0.0556, Time: 3.4s
  Epoch 7/10, Batch 200/469, Loss: 0.0345, Time: 4.5s
  Epoch 7/10, Batch 250/469, Loss: 0.0706, Time: 5.5s
  Epoch 7/10, Batch 300/469, Loss: 0.0546, Time: 6.6s
  Epoch 7/10, Batch 350/469, Loss: 0.0163, Time: 7.8s
  Epoch 7/10, Batch 400/469, Loss: 0.0993, Time: 9.0s
  Epoch 7/10, Batch 450/469, Loss: 0.0451, Time: 10.0s

Epoch 7/10 Summary:
  Average Loss: 0.0630
  Accuracy: 98.08%
  Time: 10.3s
--------------------------------------------------------------------------------
  Epoch 8/10, Batch 50/469, Loss: 0.0923, Time: 1.1s
  Epoch 8/10, Batch 100/469, Loss: 0.0856, Time: 2.1s
  Epoch 8/10, Batch 150/469, Loss: 0.0764, Time: 3.1s
  Epoch 8/10, Batch 200/469, Loss: 0.0908, Time: 4.2s
  Epoch 8/10, Batch 250/469, Loss: 0.0399, Time: 5.5s
  Epoch 8/10, Batch 300/469, Loss: 0.0528, Time: 6.7s
  Epoch 8/10, Batch 350/469, Loss: 0.0832, Time: 7.9s
  Epoch 8/10, Batch 400/469, Loss: 0.1270, Time: 9.0s
  Epoch 8/10, Batch 450/469, Loss: 0.0675, Time: 10.2s

Epoch 8/10 Summary:
  Average Loss: 0.0576
  Accuracy: 98.26%
  Time: 10.8s
--------------------------------------------------------------------------------
  Epoch 9/10, Batch 50/469, Loss: 0.0134, Time: 1.4s
  Epoch 9/10, Batch 100/469, Loss: 0.0287, Time: 2.8s
  Epoch 9/10, Batch 150/469, Loss: 0.0688, Time: 4.0s
  Epoch 9/10, Batch 200/469, Loss: 0.0372, Time: 5.2s
  Epoch 9/10, Batch 250/469, Loss: 0.0761, Time: 6.4s
  Epoch 9/10, Batch 300/469, Loss: 0.0113, Time: 7.6s
  Epoch 9/10, Batch 350/469, Loss: 0.0223, Time: 8.8s
  Epoch 9/10, Batch 400/469, Loss: 0.0321, Time: 10.4s
  Epoch 9/10, Batch 450/469, Loss: 0.0799, Time: 11.9s

Epoch 9/10 Summary:
  Average Loss: 0.0540
  Accuracy: 98.32%
  Time: 12.5s
--------------------------------------------------------------------------------
  Epoch 10/10, Batch 50/469, Loss: 0.0433, Time: 1.3s
  Epoch 10/10, Batch 100/469, Loss: 0.0527, Time: 2.6s
  Epoch 10/10, Batch 150/469, Loss: 0.0378, Time: 3.9s
  Epoch 10/10, Batch 200/469, Loss: 0.1195, Time: 6.2s
  Epoch 10/10, Batch 250/469, Loss: 0.1214, Time: 7.6s
  Epoch 10/10, Batch 300/469, Loss: 0.0644, Time: 8.9s
  Epoch 10/10, Batch 350/469, Loss: 0.0534, Time: 10.2s
  Epoch 10/10, Batch 400/469, Loss: 0.0545, Time: 11.6s
  Epoch 10/10, Batch 450/469, Loss: 0.0302, Time: 13.2s

Epoch 10/10 Summary:
  Average Loss: 0.0511
  Accuracy: 98.45%
  Time: 13.6s
--------------------------------------------------------------------------------

✓ Training completed!
  Final Accuracy: 98.45%
================================================================================

================================================================================
Evaluating CNN on Test Dataset
================================================================================
  Processed 20/79 batches...
  Processed 40/79 batches...
  Processed 60/79 batches...

✓ Evaluation completed!
  Test Accuracy: 98.36%
================================================================================

✓ Model saved to: D:\GitHub\transition-matrix-lie-group\outputs\data\cnn_mnist.pth
✓ Target accuracy (>98%) achieved: 98.36%

================================================================================
Step 3: Extracting Formal Model Features (A)
================================================================================

================================================================================
Extracting Penultimate Layer Features (Formal Model A)
================================================================================
  Processed 20/469 batches...
  Processed 40/469 batches...
  Processed 60/469 batches...
  Processed 80/469 batches...
  Processed 100/469 batches...
  Processed 120/469 batches...
  Processed 140/469 batches...
  Processed 160/469 batches...
  Processed 180/469 batches...
  Processed 200/469 batches...
  Processed 220/469 batches...
  Processed 240/469 batches...
  Processed 260/469 batches...
  Processed 280/469 batches...
  Processed 300/469 batches...
  Processed 320/469 batches...
  Processed 340/469 batches...
  Processed 360/469 batches...
  Processed 380/469 batches...
  Processed 400/469 batches...
  Processed 420/469 batches...
  Processed 440/469 batches...
  Processed 460/469 batches...

✓ Feature extraction completed!
  Features shape: (60000, 490)
  Labels shape: (60000,)
================================================================================

================================================================================
Extracting Penultimate Layer Features (Formal Model A)
================================================================================
  Processed 20/79 batches...
  Processed 40/79 batches...
  Processed 60/79 batches...

✓ Feature extraction completed!
  Features shape: (10000, 490)
  Labels shape: (10000,)
================================================================================

A_train shape: (60000, 490)
A_test shape: (10000, 490)

================================================================================
Step 4: Creating Mental Model Features (B)
================================================================================

================================================================================
Creating Mental Model Features (Flattened Images B)
================================================================================
  Input shape: (60000, 1, 28, 28)
  Flattened shape: (60000, 784)
================================================================================

================================================================================
Creating Mental Model Features (Flattened Images B)
================================================================================
  Input shape: (10000, 1, 28, 28)
  Flattened shape: (10000, 784)
================================================================================

B_train shape: (60000, 784)
B_test shape: (10000, 784)

================================================================================
Step 5: Computing Transition Matrix T_old
================================================================================
Using training data (A_train, B_train)
Computing transition matrix...
  A shape: (60000, 490) (Formal Model features)
  B shape: (60000, 784) (Mental Model features)
  A pseudoinverse shape: (490, 60000)
  T shape: (784, 490)

✓ T_old saved to: D:\GitHub\transition-matrix-lie-group\outputs\data\t_old_mnist.npy
  T_old shape: (784, 490)

================================================================================
Step 6: Evaluating Reconstruction on Test Set
================================================================================
Reconstructing mental features...
  A shape: (10000, 490)
  T shape: (784, 490)
  B* shape: (10000, 784)

Original B_test shape: (10000, 784)
Reconstructed B*_test shape: (10000, 784)

================================================================================
Step 7: Calculating Reconstruction Metrics
================================================================================
Calculating reconstruction metrics...
  Original shape: (10000, 784)
  Reconstructed shape: (10000, 784)
  MSE: 0.068354
    Processed 1000/10000 images...
    Processed 2000/10000 images...
    Processed 3000/10000 images...
    Processed 4000/10000 images...
    Processed 5000/10000 images...
    Processed 6000/10000 images...
    Processed 7000/10000 images...
    Processed 8000/10000 images...
    Processed 9000/10000 images...
    Processed 10000/10000 images...
  SSIM: 0.199759 ± 0.073069
  PSNR: 10.36 ± 0.79 dB

✓ Metrics saved to: D:\GitHub\transition-matrix-lie-group\outputs\results\baseline_metrics.json

================================================================================
BASELINE REPRODUCTION SUMMARY
================================================================================
CNN Test Accuracy: 98.36%
T_old shape: (784, 490)

Reconstruction Metrics:
  MSE: 0.068354
  SSIM: 0.199759 ± 0.073069
  PSNR: 10.36 ± 0.79 dB

Output Files:
  - D:\GitHub\transition-matrix-lie-group\outputs\data\cnn_mnist.pth
  - D:\GitHub\transition-matrix-lie-group\outputs\data\t_old_mnist.npy
  - D:\GitHub\transition-matrix-lie-group\outputs\results\baseline_metrics.json
================================================================================

✓ Baseline reproduction completed successfully!
